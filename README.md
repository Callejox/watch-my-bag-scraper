# Watch MY Bag Scraper

**Repositorio:** https://github.com/Callejox/watch-my-bag-scraper

AplicaciÃ³n de monitoreo y scraping de relojes de lujo en marketplaces online (Chrono24, Vestiaire Collective, Catawiki).

Detecta automÃ¡ticamente ventas comparando inventarios diarios y proporciona un dashboard interactivo para visualizar los datos.

## ðŸ“– Â¿Nuevo en ProgramaciÃ³n?

Si no tienes experiencia tÃ©cnica, consulta la **[GuÃ­a RÃ¡pida para Usuarios](GUIA_RAPIDA.md)** con instrucciones paso a paso simplificadas.

---

## Requisitos Previos

### OpciÃ³n A: Docker (Recomendado)
- Docker Desktop instalado
- Docker Compose

### OpciÃ³n B: InstalaciÃ³n Local
- Python 3.9 o superior
- pip (gestor de paquetes de Python)

---

## InstalaciÃ³n RÃ¡pida

### 1. Descargar el proyecto

```bash
# Clonar o copiar el proyecto a tu mÃ¡quina
cd /ruta/donde/quieras/el/proyecto
```

### 2. Ejecutar el script de configuraciÃ³n

```bash
# Dar permisos de ejecuciÃ³n (solo la primera vez)
chmod +x setup.sh

# Ejecutar configuraciÃ³n inicial
./setup.sh
```

El script automÃ¡ticamente:
- Crea los directorios necesarios (`data/`, `logs/`, `data/exports/`, `data/images/`)
- Detecta si tienes Docker o necesitas instalaciÃ³n local
- Instala las dependencias
- Inicializa la base de datos

---

## Uso con Docker (Recomendado)

### Comandos principales

```bash
# 1. INICIALIZAR (solo la primera vez)
docker compose --profile init run --rm init

# 2. EJECUTAR SCRAPER (obtener datos de los marketplaces)
docker compose --profile scrape run --rm scraper

# 3. INICIAR DASHBOARD (visualizar datos)
docker compose up dashboard
# Luego abre: http://localhost:8501

# 4. GENERAR REPORTE EXCEL
docker compose --profile report run --rm report

# 5. DETENER TODO
docker compose down
```

### Ver logs

```bash
# Ver logs del dashboard
docker compose logs -f dashboard

# Ver logs del scraper
docker compose --profile scrape logs scraper
```

---

## Uso Local (sin Docker)

### Flujo Automatizado (Recomendado - Windows)

```powershell
# FLUJO COMPLETO (scraping + validaciÃ³n + reporte) - UN SOLO COMANDO
.\run_workflow.bat

# OPCIONES DISPONIBLES:
.\run_workflow.bat --test-mode        # Modo prueba (1 modelo, 1 pÃ¡gina)
.\run_workflow.bat --chrono24-only    # Solo Chrono24
.\run_workflow.bat --vestiaire-only   # Solo Vestiaire
.\run_workflow.bat --catawiki-only    # Solo Catawiki
.\run_workflow.bat --skip-report      # Sin generar reporte Excel
```

**El workflow hace TODO automÃ¡ticamente:**
- âœ… Verifica Docker y FlareSolverr (necesarios para Chrono24 y Catawiki)
- âœ… Ejecuta scraping de todas las plataformas activas
- âœ… Verifica integridad de datos
- âœ… Repara errores si es necesario
- âœ… Genera reporte Excel si hay ventas nuevas

### Comandos Individuales (MÃ¡s control)

```bash
# 1. ACTIVAR ENTORNO VIRTUAL (siempre antes de usar)
source venv/bin/activate          # Linux/Mac
venv\Scripts\activate             # Windows

# 2. INICIALIZAR BASE DE DATOS (solo la primera vez)
python main.py --init

# 3. EJECUTAR SCRAPER (obtener datos de los marketplaces)
python main.py --scrape

# 4. INICIAR DASHBOARD (visualizar datos)
streamlit run dashboard.py
# Luego abre: http://localhost:8501

# 5. GENERAR REPORTE EXCEL
python main.py --report

# 6. VERIFICAR INTEGRIDAD DE DATOS (IMPORTANTE despuÃ©s de scrapear)
python check_integrity.py --full

# 7. REPARAR BASE DE DATOS (si se detectan problemas)
python fix_database.py --fix-all

# 8. EJECUTAR FLUJO COMPLETO (Python directo)
python main.py --workflow
```

---

## ConfiguraciÃ³n

Edita el archivo `config.py` para personalizar:

### Modelos a buscar en Chrono24

```python
CHRONO24_MODELS = [
    "Omega de ville",
    "HermÃ¨s Arceau",
    "Omega seamaster",
    # AÃ±ade mÃ¡s modelos aquÃ­
]
```

### Vendedores a seguir en Vestiaire

```python
VESTIAIRE_SELLER_IDS = [
    "3022988",      # ID del vendedor 1
    "10125453",     # ID del vendedor 2
    # AÃ±ade mÃ¡s IDs aquÃ­
]
```

### PaÃ­ses a excluir

```python
CHRONO24_EXCLUDE_COUNTRIES = [
    "JapÃ³n",
    "Japan",
]
```

---

## Estructura del Proyecto

```
watch-my-bag-scraper/
â”œâ”€â”€ main.py              # Script principal (scraper)
â”œâ”€â”€ dashboard.py         # Dashboard web (Streamlit)
â”œâ”€â”€ config.py            # ConfiguraciÃ³n (modelos, vendedores, etc.)
â”œâ”€â”€ requirements.txt     # Dependencias Python
â”œâ”€â”€ Dockerfile           # Imagen Docker
â”œâ”€â”€ docker-compose.yml   # OrquestaciÃ³n Docker
â”œâ”€â”€ setup.sh             # Script de instalaciÃ³n
â”œâ”€â”€ README.md            # Este archivo
â”‚
â”œâ”€â”€ database/
â”‚   â””â”€â”€ db_manager.py    # GestiÃ³n de base de datos SQLite
â”‚
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ base_scraper.py      # Clase base para scrapers
â”‚   â”œâ”€â”€ scraper_chrono.py    # Scraper de Chrono24
â”‚   â”œâ”€â”€ scraper_vestiaire.py # Scraper de Vestiaire
â”‚   â””â”€â”€ scraper_catawiki.py  # Scraper de Catawiki
â”‚
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ data_processor.py    # DetecciÃ³n de ventas
â”‚   â””â”€â”€ excel_manager.py     # GeneraciÃ³n de reportes
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ inventory.db         # Base de datos SQLite
â”‚   â”œâ”€â”€ exports/             # Reportes Excel generados
â”‚   â””â”€â”€ images/              # ImÃ¡genes descargadas
â”‚
â””â”€â”€ logs/                    # Archivos de log
```

---

## Flujo de Trabajo Recomendado

### Uso diario

1. **Ejecutar el scraper** una vez al dÃ­a (preferiblemente a la misma hora):
   ```bash
   # Docker
   docker compose --profile scrape run --rm scraper

   # Local
   python main.py --scrape
   ```

2. **Revisar el dashboard** para ver inventario y ventas detectadas:
   ```bash
   # Docker
   docker compose up dashboard

   # Local
   streamlit run dashboard.py
   ```

3. **Generar reporte** cuando necesites exportar datos:
   ```bash
   # Docker
   docker compose --profile report run --rm report

   # Local
   python main.py --report
   ```

### AutomatizaciÃ³n (opcional)

Puedes programar el scraper con cron (Linux/Mac) o Task Scheduler (Windows):

```bash
# Ejemplo de cron para ejecutar diariamente a las 9:00 AM
0 9 * * * cd /ruta/al/proyecto && docker compose --profile scrape run --rm scraper
```

---

## Funcionalidades del Dashboard

El dashboard (http://localhost:8501) incluye:

### PestaÃ±as Principales

1. **Inventario**:
   - Grid visual con tarjetas de productos
   - ImÃ¡genes, precios y enlaces clickeables
   - Sistema dual de imÃ¡genes (local â†’ remota â†’ placeholder)

2. **Ventas** (Vista JerÃ¡rquica de 2 Niveles):
   - **Nivel 1 - Modelo GenÃ©rico**: Expanders por modelo (Omega Seamaster, De Ville, etc.)
     - GrÃ¡ficos comparativos de sub-modelos (barras + box plots)
     - AnÃ¡lisis de distribuciÃ³n de precios
   - **Nivel 2 - Sub-Modelos**: Expanders dentro de cada modelo
     - GalerÃ­a de fotos (hasta 6 por grupo)
     - MÃ©tricas: rango de precios, media, total, dÃ­as medio en venta
     - Tabla detallada con enlaces clickeables
   - **Selector de Plataforma**: Filtrar por "Todas" | "Chrono24" | "Vestiaire"

3. **AnÃ¡lisis**:
   - GrÃ¡ficos de ventas por plataforma
   - DistribuciÃ³n de precios
   - Ventas diarias (timeline)

4. **Datos**:
   - Tabla completa exportable a CSV
   - Todos los campos disponibles para anÃ¡lisis

### Filtros Disponibles (Sidebar)

- Plataforma (Chrono24, Vestiaire, Catawiki)
- Modelo buscado
- Rango de precio (slider)
- PaÃ­s del vendedor
- CondiciÃ³n del producto
- Rango de fechas (calendarios)
- ID del vendedor (Vestiaire)
- ID del producto (Chrono24)

---

## SoluciÃ³n de Problemas

### El scraper no obtiene datos

1. Verifica tu conexiÃ³n a internet
2. Los marketplaces pueden tener protecciÃ³n anti-bot. Espera unos minutos y reintenta
3. Revisa los logs en `logs/` para mÃ¡s detalles

### El dashboard no inicia

```bash
# Verificar que el puerto 8501 estÃ¡ libre
lsof -i :8501

# Usar otro puerto
streamlit run dashboard.py --server.port=8502
```

### Error de base de datos

```bash
# Reinicializar la base de datos
# Docker
docker compose --profile init run --rm init

# Local
python main.py --init
```

### Limpiar y reconstruir Docker

```bash
# Eliminar contenedores e imÃ¡genes
docker compose down --rmi all --volumes

# Reconstruir
docker compose build --no-cache
```

---

## Notas Importantes

### ProtecciÃ³n Anti-Bot y FlareSolverr

- **Chrono24**: Requiere FlareSolverr (Docker) para bypass de Cloudflare. Ejecutar mÃ¡ximo 1-2 veces/dÃ­a
- **Catawiki**: Requiere FlareSolverr (Docker) para bypass de Cloudflare. Ejecutar mÃ¡ximo 1-2 veces/dÃ­a
- **Vestiaire**: No requiere FlareSolverr. Puede ejecutarse 3-4 veces/dÃ­a sin problemas
- **FlareSolverr**: Se inicia automÃ¡ticamente con `run_workflow.bat` o se puede iniciar manualmente con `start_flaresolverr.bat`

### Otras Consideraciones

- El scraper incluye delays aleatorios para evitar bloqueos
- Los datos se guardan localmente en SQLite (no requiere servidor de base de datos)
- Se recomienda ejecutar `check_integrity.py` despuÃ©s de cada scraping
- Las imÃ¡genes se descargan automÃ¡ticamente durante el scraping

---

## Soporte

Si encuentras algÃºn problema:

1. Revisa los logs en la carpeta `logs/`
2. AsegÃºrate de tener la Ãºltima versiÃ³n del proyecto
3. Verifica que tu configuraciÃ³n en `config.py` es correcta

---

**Watch MY Bag Scraper** - Monitoreo inteligente de relojes en marketplaces
